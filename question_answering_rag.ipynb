{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Code a Simple Question Answering System\n",
    "This hands-on project will guide you through the process of programming, covering essential steps such as defining prompt templates, integrating language models, and implementing retrieval mechanisms to fetch information. By the end of the section, you'll have gained practical experience in building a simple yet effective question-answering solution, offering valuable insights into the mechanics of this exciting field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain lxml chromadb sentence-transformers ctransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "Visit Wikipedia and retrieve the [Wikipedia page for the Porsche 911](https://en.wikipedia.org/wiki/Porsche_911). In this simplified example, we are only loading a single page, but in practice, you have the capability to load multiple pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader 44 files\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "\n",
    "file_path= r'.\\data\\Wikipedia 911\\Porsche 911 - Wikipedia.html'\n",
    "file1 = open(file_path, encoding='utf-8')\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "    (\"h5\", \"Header 5\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on, return_each_element=None)\n",
    "html_header_splits = html_splitter.split_text(file1.read())\n",
    "print(\"Loader {} files\".format(len(html_header_splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents and save them into a vector database\n",
    "In the subsequent phase, we divide the HTML page into subsections according to the headers. Feel free to employ alternative criteria for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "# Define the Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 384,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "#Create a split of the document using the text splitter\n",
    "splits = text_splitter.split_documents(html_header_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Store embeddings in a vector database.\n",
    "Next, we save all the data as word embeddings in ChromaDB. The embedding function() utilizes a transformer model loaded from the SBert library. You have the flexibility to experiment with various models. For reference, check the available options in the Pretrained Models section of the Sentence-Transformers documentation at [sbert.net](https://www.sbert.net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 Embeddings are loaded in the Vector Database\n"
     ]
    }
   ],
   "source": [
    "# Embedd the Splits\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_function, #embedding,\n",
    ")\n",
    "\n",
    "print(f'{vectordb._collection.count()} Embeddings are loaded in the Vector Database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the local LLM model\n",
    "\n",
    "Load a local LLM to answer the question. This can take a few minutes because the model will be downloaded the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d2dc3641a74095afb20bf9eff6b3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eaf27e04aa48b8bc8cb18e7b402c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import CTransformers\n",
    "# if you have a graphic card increase attribute gpu_layers\n",
    "config = {'gpu_layers':0, 'temperature':0.6, \"max_new_tokens\": 1024, \"context_length\": 4096}\n",
    "llm = CTransformers(model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_type='llama', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's bring everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which Porsche has the highest top speed? \n",
      "Results  The Porsche 911 GT3 RS has the highest top speed among the 997 lineage. It can reach speeds of up to 340 km/h (210 mph). \n",
      "Source: [Document(page_content='Unlike the previous GT2 versions, this car is fitted with a 7-speed PDK transmission to handle the excessive torque produced from the engine. Porsche claims that this car will accelerate from 0 to 60\\xa0mph (97\\xa0km/h) in 2.7\\xa0seconds, and has a top speed of 340\\xa0km/h (210\\xa0mph).', metadata={'Header 1': 'Porsche 911', 'Header 2': 'Water-cooled engines (1998–present)[edit]', 'Header 3': '991 Series (2011 – 2019)[edit]', 'Header 4': '991 GT2 RS[edit]'}), Document(page_content='magazine tested the 997 GT3 and recorded a 0–100\\xa0km/h (62\\xa0mph) of 3.9\\xa0seconds and a top speed of 312\\xa0km/h (194\\xa0mph). It was at that time crowned \"the best handling car in America\" by Motor Trend.[citation needed]', metadata={'Header 1': 'Porsche 911', 'Header 2': 'Water-cooled engines (1998–present)[edit]', 'Header 3': '997 (2004–2013)[edit]', 'Header 4': '997 GT3[edit]'}), Document(page_content=\"The 911 GT3 was added to the 997 lineage on 23 February 2006. Performance figures include a 0–100 kilometres per hour (0–62\\xa0mph) acceleration time of 4.1\\xa0seconds and a top speed of 310\\xa0km/h (193\\xa0mph), almost as fast as the Turbo. Porsche's factory reports were proven to be conservative about the performance of the car; Excellence magazine tested the 997 GT3 and recorded a\", metadata={'Header 1': 'Porsche 911', 'Header 2': 'Water-cooled engines (1998–present)[edit]', 'Header 3': '997 (2004–2013)[edit]', 'Header 4': '997 GT3[edit]'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.  \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "results = qa_chain.invoke({\"query\": \"Which Porsche has the highest top speed?\" })\n",
    "print('Query: {} \\nResults {} \\nSource: {}'.format(results['query'], results['result'], results['source_documents']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdac7f044e5837e87ecce10037448baaa49131c68b86e233dd601474cdad7df0"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "undefined.-xfrozen_modules=off"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
