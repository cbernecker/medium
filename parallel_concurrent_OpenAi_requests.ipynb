{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Scoring with OpenAI \n",
    "You can use a similar multithreading approach to make batch requests to the OpenAI API's completions endpoint. This can be particularly useful if you need to process a large number of requests in parallel to improve throughput and reduce waiting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary packages\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Decorator to add multithreading\n",
    "def multithreaded(max_workers=5):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                future_to_args = {executor.submit(func, arg): arg for arg in args[0]}\n",
    "                results = []\n",
    "                for future in as_completed(future_to_args):\n",
    "                    arg = future_to_args[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                    except Exception as exc:\n",
    "                        print(f'{arg} generated an exception: {exc}')\n",
    "                    else:\n",
    "                        results.append(result)\n",
    "                return results\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses: ['Error: Connection error.', 'Error: Connection error.']\n",
      "Time taken: 2.7098333835601807 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os \n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client.api_key = os.environ[\"OPENAI_API_KEY\"] #'your-api-key-here'\n",
    "\n",
    "# Function to make a request to the OpenAI API\n",
    "@multithreaded(max_workers=5)\n",
    "def get_completion(prompt):\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=50,\n",
    "        temperature=1.1\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# List of prompts to process\n",
    "prompts = [\n",
    "    \"Tell me a joke.\",\n",
    "    \"What's the weather like today?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Write a short story about a dragon.\",\n",
    "    \"Translate 'Hello' to Spanish.\",\n",
    "    \"What's the tallest mountain in the world?\",\n",
    "    \"Who was Albert Einstein?\",\n",
    "    \"Give me a recipe for chocolate cake.\",\n",
    "    \"What are the benefits of meditation?\"\n",
    "]\n",
    "\n",
    "# Using the decorated function to process prompts in parallel\n",
    "start_time = time.time()\n",
    "responses = get_completion(prompts)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Responses:\", responses)\n",
    "print(\"Time taken:\", end_time - start_time, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "undefined.-xfrozen_modules=off"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
